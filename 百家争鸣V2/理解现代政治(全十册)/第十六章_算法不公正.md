# 第十六章_算法不公正

理解现代政治（全十册）（严密论证，透彻分析，廓清现实，直面复杂 理想国出品）

第十六章

算法不公正

我们不知道为何这个世界充满苦难。但是，我们可以知道，世界是如何决定痛苦将降临于某些人而不是其他人的。

——圭多·卡拉布雷西和菲利普·博比特，

《悲剧性选择》（1978）

在数字生活的世界里，社会工程和软件工程将变得越来越难以区分，有两个原因。首先，随着时间的推移，在市场和国家体系里，算法逐渐会被用于确定重要社会物品的分配，包括工作、贷款、住房和保险等。其次，在法律法规允许的范围下，算法在对人的识别、排序、分类和管理方面的应用也会越来越多。分配和承认作为社会正义的实质，将来会被逐渐托付给代码。这是本书以上两章的主题。更委婉地说，这将是人类政治生活的重要发展。这意味着可以用代码来减少不公正，代码也可以让旧的不公正沉渣泛起，也可能创造新的正义。在数字生活世界中，正义在很大程度上取决于其所使用的算法及应用算法的方式。我之所以谈到算法的应用，是因为算法和数据的结合经常会产生不公正的结果，这种情况不是算法一方造成的。下文将进一步解释此说法。

接下来，我将花一点篇幅来为思考算法不公正提供广泛框架。算法不公正即指算法的应用会产生不公正的结果。我们从两种主要的算法不公正开始：基于数据的不公正和基于规则的不公正。然后，我们检视我称其为“中立谬误”的东西，这个有缺陷的观念认为，我们需要的是中立或公正的算法。在本章的最后，我们会发现，隐藏在所有技术背后的大多数算法不公正，实际上可以追溯到人的行为和决定——从软件工程师到使用谷歌搜索的用户，都可能是其中的一分子。

速成测试

在讨论细节之前，有一种简单的方法可以判断某个算法的特定应用公不公正：它提供的结果是否符合相关的正义原则？以用于计算医疗保险费的算法为例。譬如，为了符合充足主义者的原则，这种算法必须将资源集中在社会最贫困的那部分人身上，确保他们有低保。相反，如果它向穷人普遍征收更高的保险费用，那么很明显，它最终将使穷人更难获得保险。因此，从充足主义者的角度看，这种应用就是不公正的。多么简单！这个粗略、方便的测试采用了一种结果导向的方法，它并不试图评估代码的应用在本质上是善良的或是正确的，也不需要对算法本身进行严密的技术分析，只需要评估某种算法的应用产生的结果是否能与给定的正义原则相一致。这只是评估算法不公正的一种方法。政治理论家的任务之一就是要发现更多的评估方法。

“算法区分”

不同类型的算法不公正有时被集中在一起，统称为“算法区分”（algorithmic discrimination）。我一般避免使用该词和“算法偏见”（algrithmic bias），因其会令人不解。区分（discrimination）是一个微妙的概念，至少有三个可接受的含义。第一个是中性的，指的是区分一件事物和另一件事物的过程。［如果我说你是一个极有眼光的（discriminating）艺术评论家，我是在称赞你的敏锐，而不是在说你偏执。］第二个是指在群体之间明显不公平的区分，比如父亲拒绝让自己的孩子与其他种族的孩子交往。第三个则是法律意义上的，指违反禁止给予特定群体较差待遇的某一特定法律的规则或行为，即歧视。作为一名律师，我经常处理涉嫌歧视指控的案件。

从上述不同可以看出，并不是所有的区分都是不公正的，也不是所有不公平都是非法的。例如，在英国法律里，雇主不能歧视雇员的年龄、残疾、变性、怀孕生育、种族、宗教信仰、性别和性取向等，但他们可以根据阶层区别对待（只要不违反其他法律）。基于阶层的区分并不违法，但它仍然是不公正的。这种区别非常重要，因为记者和律师往往把算法不公正的问题简化为法律上的歧视问题，美国或欧洲法律允许这种代码应用吗？尽管这类问题很重要，但政治理论的方法更为广泛。我们不仅需要问什么是合法的，还要问什么是公正的.不仅要知道法律是什么，还要知道它应该是什么。所以让我们暂退一步，从更宏观的角度来考察这个问题。

算法的不公正主要有两类：基于数据的不公正和基于规则的不公正。

基于数据的不公正

当一种算法应用于选择不当、不完整、过时或存在选择偏见的数据时，不公正就出现了。

[1]

不良数据的问题对机器学习算法来说尤其突出，因为机器只能依据其面对的数据来学习。例如为人脸识别而训练的算法，如果它的训练集主要是白人面孔，那么在遇到非白人面孔时，机器就很难或者根本识别不出来。

[2]

如果语音识别算法是从包含大量男性声音的数据集中训练出来的，那么它将无法辨识女性声音。

[3]

如果训练时接触的主要是白人面孔，那么就连一种可以根据面部对称、皱纹和年轻程度等所谓“中立特征”来判断人是否美貌的算法，也会发展出对白种人特征的喜好。在最近的一次竞赛中，来自世界各地的60万名参赛者发送了自拍照，由机器学习算法进行评判。在被认为最具吸引力的44张面孔中，只有6张不是白人，而其中肤色明显偏黑的只有一人。

[4]

一个名叫Flickr的图片网站将黑人照片自动标记为“动物”和“猿猴”，将集中营的照片打上了“运动”和“攀爬架”的标签。

[5]

谷歌的照片算法竟把两个黑人标记为“大猩猩”。

[6]

不管算法有多聪明，若给它灌输的都是对世界的片面或误导性的看法，它就不会公正地对待那些被隐藏在其视野之外或光线暗淡处的人。这就是基于数据的不公正。

基于规则的不公正

即使没有数据选择不佳、不完整、过时或选择偏见的影响，若算法应用的规则不公正，也会产生不公正。这包括两种类型：显性不公正的和隐性不公正的。

显性不公正的规则

显性不公正的规则是指：根据表面上看起来就不公正的标准来决定有关分配和承认的问题。某机器人服务生被编入拒绝为穆斯林服务的程序，只因对方是穆斯林.某安全系统被编入针对黑人的程序，只因为对方是黑人.某简历处理系统被写入拒绝女性申请者的程序，只因对方是女性。这些都是显性不公正的标准。它们之所以不公正，是因为挑出来的个人特征（宗教、种族、性别）和其导致的分配或承认被剥夺的结果（一盘食物、进入某建筑物的许可、一份工作）之间没有原则性关联。

显性不公正的规则在涉及种族和性别等特征时最为明显，这些特征在过去是典型的压迫依据，与该规则的适用背景无关。然而，导致不公正的其他标准还有很多。以相貌丑陋为例，如果我拥有一家夜总会，它安装有扫描人脸的自动门禁系统（叫它“机器保镖”好了），够漂亮的人才准入内，这样算不算不公平？现实生活中的保镖也总这么做。如果应聘者具备胜任这份工作的资格，而招聘算法却基于应聘者的信用分数将其拒绝，这种做法是否有失公允？反过来说，如果某人是Facebook上某有钱人的朋友，信用评分算法就去给他更高的评分，这公平吗？

[7]

根据现行法律，这些例子可能并不构成歧视，但仍可以说它们是不公正的，因为它们依据标准，而非参照与个人直接相关的属性，来确定人们是否能够获得一项重要社会福利。

规则在很多方面都可能是显性不公正的。这里有一个任意性的问题，即所应用的标准和所寻求的东西之间无关。或者它们违反了“群体属性谬误”（group membership fallacy）：我属于一个群体，而这个群体往往具有某种特征，但这并不意味着我一定也有那种特征（这一点在概率的机器学习方法中时常被忽略）。还有一个根深蒂固的问题：来自高收入家庭的学生在大学取得更好成绩的可能性更大，但以家庭收入作为录取标准显然会加深已然存在的教育不平等。

[8]

这是一种因果关系谬误：数据或许会显示，打高尔夫的人往往在生意上更成功，但这并不意味着他们生意的成功是打高尔夫带来的（在此类基础上的招聘很可能与正义原则相龃龉，因为正义原则认为招聘应该择优录取）。这只是其中几个例子，鉴于我们对人类无知和偏见的了解，它们显然只是冰山一角。

隐性不公正的规则

隐性不公正规则是指：不直接单独粗暴对待任何特定个人或群体，而是间接地使某些群体受到不如其他群体的待遇。如招聘规则要求应聘者身高必须超过1.8米，喉结突出，尽管它并没有提及性别，但显然是对女性更不利的。

隐性不公正的规则有时被用作公开的性别歧视或种族主义的遮羞布，但不公正这种副作用总归不是人们乐于接受的。想象一下，一个软件工程师的招聘算法会优先考虑18岁之前就开始编程的人。如果你和大多数人一样相信，早年的学习经验是日后熟练度的良好指标，那么这个规则似乎是合理的。它并没有直接挑出任何社会群体给予较差待遇，因此它不是显性不公正的。但在实践中，这个规则就可能会损害女性候选人的前途，因为文化和代际因素，她们年轻时或许没有接触过计算机科学。同时，它还会限制年龄较大的候选人的机会，因为他们小时候家里甚至没有个人电脑。因此，一个看似合理的规则可能会间接地使某些群体处于不利地位。

[9]

不公正的规则在形式上可能很微妙。让我们回顾一下此前在基于数据的区分部分讨论过的人脸识别算法。这些算法在数字生活世界中将会很常见，因为强大的系统每天都在识别我们，并与我们互动。想象一下与数字系统互动时的可怖场景，它可能由于色差、疤痕或毁容等特征的变化而无法识别你的脸。可见，即使是定位人脸这种小事，也可能危机四伏，更不用说确定这张脸到底属于哪个人了。一种方法是使用算法定位“或明或暗的‘斑点’图案”，来表示眼睛、颧骨和鼻子。但是，由于这种方法有赖于脸部颜色和眼白之间的色彩对比，光线问题可能会导致较难识别某些种族。而“边缘检测法”（edge detection）则试图找出人脸与其周围背景之间的区别。这仍然会造成问题，因为它取决于人脸的颜色和背景的颜色。还有一种方法是根据预先写入的肤色调色板来检测人脸。这种技术或许不会产生冒犯性结果，但它需要程序员在定义什么颜色是“肤色”时具有高度的敏感性。

[10]

你可能疑惑，这有什么好大惊小怪的？但若由于种族原因，人们一直无法识别某个群体，这就会被认为是一种明显的不承认，也是对该群体尊严的侮辱。被机器和人无礼对待，究竟哪一个更糟糕，还不太容易回答。不过，无论机器还是人，皆事关正义。

再举一个棘手的例子。“普林斯顿评论”提供SAT在线辅导课程，这款软件根据学生所在地邮政编码收取不同的费用。其目的似乎是让富人支付更多的私人学费。在相对贫困的地区，课程费用可能是6000美元，而在更富裕的地区，费用可能高达8400美元。从表面上看，根据公认的正义原则，这一规则可能是合理的，它优先考虑较不富裕的人群，看上去是在鼓励机会平等。但它也有一个副作用，就是对亚裔美国学生收取更高的学费。从统计数据来看，亚裔美国学生往往集中在较富裕的地区，因此对他们收取更高学费的可能性几乎是其他群体的两倍。

[11]

这个事例的棘手之处在于，它需要在两个相互冲突的正义原则之间进行权衡：是给予穷人教育优势更重要，还是避免差别对待某些族群更重要？你的答案可能与我的答案不同，这都很正常。

上一段的例子有效地提醒我们，并非所有区分群体的规则都必然是不公平的，即使它们使得某些群体的待遇不如其他群体。有时，根据正义原则，“区分”可以被证明是正当的。几年前，在我决定自己的人生方向时，曾考虑过参军。像许多头脑发热的年轻人一样，我被英国精英团——英国特种空勤团（SAS）深深地吸引住了。我拿着笔和记事本，满怀激动地坐下研究SAS的招聘网站。我从网站上了解到，很多人都想加入空军特种部队，而他们中的大多数都失败了。艰难的筛选过程将把大多数人都淘汰下去，平均每125名候选人中，最后留下的只有10人。“太棒了，”我对自己说，“这听起来很有挑战。”继续读下去，我了解到这个选拔过程包括三个阶段。第一项是“耐力”，一项为期三周的体能和生存测试，要背着25公斤的背包，跋涉65千米，穿越臭名昭著的恶劣地形——南威尔士的布雷肯山和布莱克山。幸存下来的人就能进入伯利兹的丛林训练阶段，这个阶段“将淘汰那些不能遵守纪律的人”，以便“选出能在高压严酷的环境中连续工作数周的人”。听上去挺不错的。伯利兹阶段过后，学员们可以放松下来，因为他们知道，该过程的第三个阶段，即逃生、避险和战术讯问，只需顶住白噪声轰炸下残忍的审讯，并以“压力姿势”（stress positions）立正数小时就能过关。该网站耐心地解释道，“女性审讯者”甚至会“嘲笑其审问对象的男子气概”。

因此，我放弃参军，成了一名律师。

英国特种空勤团的招募过程显然体现了“区分”的中性含义。最重要的是，它将“真的猛士”和“沙发土豆”区别开来，后者希望不费什么力气就能穿上酷酷的制服。更直接地说，在寻找具备必要素质的“男性”时，它也完全排除了妇女在部队服役的机会。第一个区分显然是一种合法的“区分”形式。第二个则是有争议的：有些人可能会说，如果能通过这些阶段的考验，女性为什么不能加入SAS呢？如今，世界各地的军队都有女战士的身影。除了大多数律师都是糟糕的士兵这一事实，我还认为，对区分的识别只是对话的开始。这就是一再说这个术语并不总是有用的原因。对于隐性或显性不公正的规则，真正关键的始终是其结果能否根据正义原则被证明。

中立谬误

关于算法，最令人沮丧的事情之一是，即便其应用规则刻意在群体之间保持中立，它仍然可能导致不公平。为什么会这样？因为中立规则会重复和巩固世界上已经存在的不公正。

如果你在谷歌上搜索一个非洲裔美国人式的名字，你很可能会看到instantcheckmate.com的广告，这是一个提供犯罪背景调查的网站。如果你输入的名字不像非洲裔美国人的名字，该网站就不会弹出。

[12]

这是为什么呢？可能是因为谷歌或instantcheckmate.com应用了一项显性不公正的规定，即非洲裔美国人式的名字应该触发犯罪背景调查的广告。不出所料，谷歌和instantcheckmate.com都强烈否认这一点。那么究竟发生了什么呢？尽管我们不能确定，但也可以推测，谷歌是通过应用一个中性规则来决定应该显示哪个广告的：如果输入搜索词X的人倾向于点击广告Y，那么广告Y就应该更加突出地显示给那些输入搜索词X的人。这种操作带来的不公正，并非由显性不公正的规则或低质量数据引起：我们得到种族主义的结果，是因为人们之前的搜索和点击就显示了种族主义的模式。

如果你使用谷歌的自动完成系统，也会出现类似情况，该系统会根据输入的前几个单词提供完整的问题。如果你输入“为什么同性恋……”，谷歌就会提供若干完整的问题，如“为什么同性恋有奇怪的声音”。一项研究表明，在自动完成的有关黑人、同性恋和男性的问题中，本质上是“负面的”问题“占比相对较高”：

对黑人来说，这些问题包含了将他们塑造为懒惰、犯罪、欺诈、成绩不佳以及患有各种疾病（如皮肤干燥或肌瘤）形象的描述。同性恋者则被冠以感染艾滋病、下地狱、不配享有平等权利、声音高或说话像女孩等消极的描述。

[13]

这些都是算法不公正的典型例子。宣扬对某些群体的负面刻板印象的系统谈不上平等地对待和尊重他们。它带来的后果也必然体现在分配上。例如，男性看到的高收入职位广告比女性看到的要多。这无疑意味着男性经济机会的增加，而女性的经济机会在减少。

[14]

在这些案例中出现的情况是，应用在统计学上具有代表性的数据的“中性”算法，似乎已经重现了世界上已经存在的不公正现象。谷歌算法将“为什么女人……”的问题自动补充为“为什么女人说话那么多”，因为有很多用户曾问过这个问题。它为我们的偏见提供了一面镜子。

再举一组不同的例子，随着时间的推移，这些例子的重要性可能会越来越显著：“声誉系统”（reputation system）根据他人的评价来帮助确定人们获得社会物品（如住房或工作）的机会。Airbnb和Uber是“共享经济”的领头羊，它们依赖的就是这种声誉系统。当然，也有对教授、酒店、租户、餐馆、书籍、电视节目和歌曲以及其他任何能够量化的东西进行评级的方法。声誉系统的意义在于，它允许我们根据其他人对同一陌生事物的评价来做出判断。正如汤姆·斯利（Tom Slee）所说：“声誉是对他人评价的社会升华。”

[15]

相比于一个二星级的Airbnb房东，你当然更倾向于相信一个五星级的房东。

声誉系统相对较为年轻，也可能会在数字生活世界中变得更加普遍。reputation.com已经开始提供能帮助你获得更好分数的服务。

[16]

我认为，在数字生活世界中，我们获得商品和服务的机会可能最终取决于他人对我们的看法。回顾一下发生在中国的例子：30多个地方政府正在编制公民的社会和金融行为的数字记录，以便对他们进行评分，这样一来，“值得信赖的人”就可以“走遍天下”，而“信誉不好的人”就会“寸步难行”。

[17]

一般来说，汇总和总结人们评分的算法是中立的。对人们进行评级时，该算法只是将评分汇总成一个综合分数。问题是，即使算法是中立的，也有充分证据显示，真正打分的人不是中立的。

[18]

一项研究表明，在Airbnb上，有着明显非裔美国人名字的房客的申请接受概率比明显是白人名字的房客低16%。无论是大业主还是小业主，从个人业主到拥有房地产投资组合的大企业，情况都是如此。

[19]

正如汤姆·斯利所言：

[20]

声誉……无论约翰是多么值得信赖，如果他是一个想在种族主义历史渊源已久的白人社区找工作的黑人，他都很难获得良好的声誉.同样，如果一个社区依然秉持着传统观念去看待女性，那么简在管道修理方面的技能也很难得到认真对待。

因此，中立算法可能会重现世界上已经存在的不公正，并将其加以制度化。

随着时间的推移，一直在学习人类的数字系统将会学到人类身上哪怕上最不显眼的不公正。最近，一个神经网络算法在一个存储着300万个英语单词的数据库上学会了回答简单的类比问题。“巴黎之于法国，正如东京之于？”该系统给出了正确回答（日本）。但当被问“男人之于计算机程序员，正如女人之于什么”时，系统的答复竟是家庭主妇。问“父亲之于医生，正如母亲之于什么”，系统的回答为护士。“他”之于建筑师正如“她”之于室内设计师。这项研究揭示的东西令人震惊，但转念一想，其实又没什么可惊讶的，人类使用语言的方式反映了不公正的性别刻板印象。只要数字系统学习的对象是有缺陷的、乱作一团的和不完美的人类，我们就可以预期：中立算法将导致更多的不公正。

[21]

这些例子令人不安，因为它们挑战了一种许多人（我注意到在科技圈尤其多）都有的本能：如果它对待每个人都一样，那么它就是公正的。我称这种看法为“中立谬误”。公允地说，它的历史已经很悠久了。这种理念源于普适性的启蒙理想，即在政治的公共领域中，人与人之间的差异应被视为无关紧要的。

[22]

这一理想逐渐演变成一种当代信念，即人与人之间、群体与群体之间的规则应该是公正的。艾利斯·玛丽昂·扬在20世纪末把“公正”描述为当时“道德理性的标志”。

[23]

那些不假思索地接受中立谬误的人倾向于认为，代码为正义提供了一个激动人心的前景，因为代码可以施行不具人格的、客观的和不带感情色彩的规则。他们认为，代码没有激情、偏见和意识形态承诺，这些东西潜伏于每个不完美的人类心中。数字系统可能最终会提供哲学家们寻求已久的“本然的观点”（view from nowhere）。

[24]

然而，荒谬之处在于，中立并不总是等同于正义。诚然，在某些情况下，在群体间保持中立是很重要的，譬如法官在针对同一事件的两个相互冲突的叙述中做出决定时。但上述例子表明，像对待所有人一样对待弱势群体，实际上会复制、巩固甚至产生新的不公正。诺贝尔和平奖得主德斯蒙德·图图（Desmond Tutu）曾说：“如果大象把脚踩在老鼠的尾巴上，你说你是中立的，那么老鼠不会欣赏你的中立。”他的意思是，一个中立的规则很容易就会成为一个不公正的规则。然而，中立谬误给这些不公正的事例披上客观的外衣，会使情况变得更糟（它们看起来是如此自然且不可避免），但实际上并非如此。

技术专家的教训是，正义时常要求区别对待不同的群体。这个想法是平权行动和资助少数民族艺术的基础，它也应该成为我们避免算法不公的所有努力的基础。对代码应用的评判，应该看其所产生的结果是否符合相关的正义原则，而不是看所应用的算法在人与人之间是否保持了中立。诺贝尔奖得主埃利·威塞尔（Elie Wiesel）认为：“中立帮助的是压迫者，而不是受害者。”

编码良好的社会

想想数字生活世界中海量的代码及其将被赋予的巨大责任，还有它在社会和经济生活中发挥的广泛作用吧！算法的不公正似乎已经弥漫到了社会的各个角落，我们已经习惯了在线申请表的填写、超市自助结账系统、机场的生物识别护照门、智能手机指纹扫描仪和Siri、Alexa等早期的人工智能小助理。但未来会出现更加先进的数字系统，我们与它们的日常互动会变得无穷无尽。许多数字系统将拥有物理的、虚拟或全息的实体存在。有些数字系统也会具备人类或动物的特质，旨在建立与人的同理心和融洽关系。

[25]

因此，当此类系统对我们缺乏尊重、忽视甚至侮辱我们时，我们受到的伤害就会更大。

随着代码范围和职权的扩大，算法不公正的风险也在增加。

如果我们要赋予算法在分配和承认方面更多的控制权，那么就需要保持警惕。然而，要了解某个特定的代码应用为何会导致不公正往往并不容易。过去，歧视的意图隐藏在人们的心中.未来，它也可能会隐藏在规模和复杂性惊人的机器学习算法中.它还可能被锁在一个代码“黑箱”中，受到保密法的保护。

[26]

另一个困难是，潜在的不公似乎无处不在，它们潜藏在糟糕的数据中，埋伏于不公正的规则中，甚至中立的规则中也有它们的魅影，等待着向我们张开魔爪的机会。这种情况令人遗憾。然而，创造新世界的责任也落到了人类肩上，在这个新世界里，代码是机会的引擎，而非不公正的引擎。我们能很容易将算法，尤其是机器学习算法，当作能够通过其自身道德行为能力挣脱实体的力量。事实并非如此。机器能够不断“学习”的事实并不能免除我们“教导”它们区分正义与非正义的责任。除非人工智能系统独立于人类控制而存在，即便到那时，监控和防止算法不公正的也许还是人类自己。这项工作不能留给律师和政治理论家去做，担子将逐渐落在收集数据、构建系统和应用规则的人的肩头。不管你喜欢与否，软件工程师将越来越多地成为数字生活世界的社会工程师。这是一个巨大的责任。代码的不公正应用时常会潜入数字系统，因为工程师们没有意识到自己的个人偏见（这并不一定是他们的错。计算机科学学位的弧线很长，但它不一定会向正义倾斜）。有时，这可能是企业文化和价值观中更广泛问题的结果。至少，当学习后的机器提出规则和模型时，它们的输出就必须接受仔细检查，以确定它们在这种情况下是否存在显性或隐性的不公正。如果不这样做，就会导致算法的不公正。在第十九章中，我们将探讨一些可能避免这种不公正的其他措施，包括对科技公司的监管和对算法的审计。但是，为什么不能有意识地在设计系统时把正义放在心上呢？无论是平等的待遇、机会平等，还是其他适用于这个特定应用的原则。代码可以为正义提供令人兴奋的新前景，而不仅仅是另一个令人担忧的威胁。

我们需要蒂姆·伯纳斯—李所设想的一代“哲学工程师”，这一群体必须比现在更加多样化。把事关正义的算法托付给一个绝大多数由男性组成的工程师团体是不正确的。

[27]

非洲裔美国人拥有约10%的计算机科学学位，约占劳动力总数的14%，但在硅谷计算机行业中，非洲裔美国人所占比例才不到3%。

[28]

至少，劳动力群体若更具公众代表性，可能意味着人们对任何特定代码应用程序的社会影响会有更加深刻的认识。

现在，是时候放下算法的不公正，转向数字生活世界社会正义的另一个潜在挑战了：技术导致的失业。